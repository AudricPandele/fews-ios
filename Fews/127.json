[{
   "_id": "1453914367",
   "_rev": "2-cc7dc4f8fe9c84d5bb36f99e67850a57",
   "articles": [
       {
           "origin": {
               "url": "http://feeds.bbci.co.uk/news/technology/rss.xml",
               "name": "bbc"
           },
           "timestamp": 1453936165,
           "top_image": {
               "original": "http://ichef-1.bbci.co.uk/news/1024/cpsprodpb/973C/production/_87961783_87961779.jpg"
           },
           "link": "http://www.bbc.co.uk/news/technology-35420579#sa-ns_mchannel=rss&ns_source=PublicRSS20-sa",
           "title": "Google achieves AI 'breakthrough' by beating Go champion"
       },
       {
           "origin": {
               "url": "http://www.zdnet.com/news/rss.xml",
               "name": "zdnet"
           },
           "timestamp": 1453960966,
           "top_image": {
               "original": "http://zdnet3.cbsistatic.com/hub/i/r/2014/10/02/952c0a7a-49e7-11e4-b6a0-d4ae52e95e57/thumbnail/770x578/af1c7c0aa9dd3b122597ef8a5ed01dd5/datacentre-crop.jpg"
           },
           "link": "http://zdnet.com.feedsportal.com/c/35462/f/675634/s/4d2e5e84/sc/3/l/0L0Szdnet0N0Carticle0Cgoogle0Ealphago0Eai0Eclean0Esweeps0Eeuropean0Ego0Echampion0C0Tftag0FRSSbaffb68/story01.htm",
           "title": "Google AlphaGo AI clean sweeps European Go champion"
       },
       {
           "origin": {
               "url": "http://www.extremetech.com/feed",
               "name": "extremetech"
           },
           "timestamp": 1453924801,
           "top_image": {
               "original": "http://www.extremetech.com/wp-content/uploads/2016/01/go.jpg"
           },
           "link": "http://www.extremetech.com/gaming/222040-googles-deepmind-ai-beats-humans-at-the-massively-complex-game-go",
           "title": "Google’s DeepMind AI beats humans at the massively complex game Go"
       },
       {
           "origin": {
               "url": "http://feeds.nytimes.com/nyt/rss/Technology",
               "name": "nytimes"
           },
           "timestamp": 1453932544,
           "top_image": {
               "original": "http://graphics8.nytimes.com/images/2016/01/27/technology/personaltech/27bits-hassabis/27bits-hassabis-facebookJumbo.jpg"
           },
           "link": "http://bits.blogs.nytimes.com/2016/01/27/alphabet-program-beats-the-european-human-go-champion/?partner=rss&emc=rss",
           "title": "Alphabet Program Beats the European Human Go Champion"
       },
       {
           "origin": {
               "url": "http://feeds.feedburner.com/TechCrunch",
               "name": "techcrunch"
           },
           "timestamp": 1453988836,
           "top_image": {
               "original": "https://tctechcrunch2011.files.wordpress.com/2016/01/8297692520_1993250e99_o.jpg"
           },
           "link": "http://feedproxy.google.com/~r/Techcrunch/~3/R83TWzcalgI/",
           "title": "Following Artificial Intelligence Breakthrough, European Go Champion Loses Against Computer"
       },
       {
           "origin": {
               "url": "http://www.ibtimes.com/rss/technology",
               "name": "ibtimes"
           },
           "timestamp": 1453974600,
           "top_image": {
               "original": "http://s1.ibtimes.com/sites/www.ibtimes.com/files/2016/01/28/deepmind.png"
           },
           "link": "http://www.ibtimes.com/google-deepminds-alphago-program-defeats-human-go-champion-first-time-ever-2283700",
           "title": "Google DeepMind’s ‘AlphaGo’ Program Defeats Human Go Champion For The First Time Ever"
       },
       {
           "origin": {
               "url": "http://feeds.venturebeat.com/VentureBeat",
               "name": "venturebeat"
           },
           "timestamp": 1453935653,
           "top_image": {
               "original": "http://1u88jj3r4db2x4txp44yqfj1.wpengine.netdna-cdn.com/wp-content/uploads/2016/01/Go-board-Google-2-780x439.jpg"
           },
           "link": "http://venturebeat.com/2016/01/27/googles-ai-beats-a-professional-go-player-an-industry-first/",
           "title": "Google's AI beats a professional Go player, an industry first"
       },
       {
           "origin": {
               "url": "http://feeds.gawker.com/gizmodo/full",
               "name": "gizmodo"
           },
           "timestamp": 1453959900,
           "top_image": {
               "original": "https://i.kinja-img.com/gawker-media/image/upload/s--f7-GSC_Y--/c_fill,fl_progressive,g_north,h_358,q_80,w_636/yn8erj4dtduo5chkgbdc.jpg"
           },
           "link": "http://feeds.gawker.com/~r/gizmodo/full/~3/ZGSjYuXXWMU/google-just-beat-facebook-in-race-to-artificial-intelli-1755435478",
           "title": "Google Just Beat Facebook in Race to Artificial Intelligence Milestone"
       },
       {
           "origin": {
               "url": "http://rssfeeds.usatoday.com/usatoday-TechTopStories",
               "name": "usatoday"
           },
           "timestamp": 1453955069,
           "top_image": {
               "original": "http://www.gannett-cdn.com/-mm-/9e458fbb65cd9471da0f9ef9480b8690c6960fe5/c=0-0-189-107&r=x633&c=1200x630/local/-/media/2016/01/27/USATODAY/USATODAY/635894971769002368-Go-image3.png"
           },
           "link": "http://rssfeeds.usatoday.com/~/134899181/0/usatoday-techtopstories~Google-DeepMinds-program-beats-human-at-Go/",
           "title": "Google DeepMind's program beats human at Go"
       }
   ],
   "images": [
     "http://i.kinja-img.com/gawker-media/image/upload/s--qHlCAuU1--/yn8erj4dtduo5chkgbdc.jpg",
     "http://1u88jj3r4db2x4txp44yqfj1.wpengine.netdna-cdn.com/wp-content/uploads/2016/01/Google-Nature-chess-300x394.jpg",
     "http://s1.ibtimes.com/sites/www.ibtimes.com/files/styles/lg/public/2016/01/28/deepmind.png"
   ],
   "texts": [
       "Google's DeepMind division has achieved a landmark in AI A Google artificial intelligence program has beaten the European champion of the board game Go. The Chinese game is viewed as a much tougher challenge than chess for computers because there are many more ways a Go match can play out. The tech company's DeepMind division said its software had beaten its human rival five games to nil. One independent expert called it a breakthrough for AI with potentially far-reaching consequences. The achievement was announced to coincide with the publication of a paper, in the scientific journal Nature, detailing the techniques used. Earlier on Wednesday, Facebook's chief executive had said its own AI project had been \"getting close\" to beating humans at Go. But the research he referred to indicated its software was ranked only as an \"advanced amateur\" and not a \"professional level\" player. What is Go? Media caption A brief guide to Go Go is thought to date back to ancient China, several thousand years ago. Using black-and-white stones on a grid, players gain the upper hand by surrounding their opponents pieces with their own. The rules are simpler than those of chess, but a player typically has a choice of 200 moves compared with about 20 in chess. There are more possible positions in Go than atoms in the universe, according to DeepMind's team. It can be very difficult to determine who is winning, and many of the top human players rely on instinct. DeepMind's chief executive, Demis Hassabis, said its AlphaGo software followed a three-stage process, which began with making it analyse 30 million moves from games played by humans. \"It starts off by looking at professional games,\" he said. Media caption Demis Hassabis explains how DeepMind achieved the computing milestone. \"It learns what patterns generally occur - what sort are good and what sort are bad. If you like, that's the part of the program that learns the intuitive part of Go. \"It now plays different versions of itself millions and millions of times, and each time it gets incrementally better. It learns from its mistakes. \"The final step is known as the Monte Carlo Tree Search, which is really the planning stage. \"Now it has all the intuitive knowledge about which positions are good in Go, it can make long-range plans.\" Tested against rival Go-playing AIs, Google's system won 499 out of 500 matches, And last October, DeepMind invited Fan Hui, Europe's top player, to its London office for a series of games, each of which the AI won. \"Many of the best programmers in the world were asked last year how long it would take for a program to beat a top professional, and most of them were predicting 10-plus years,\" Mr Hassabis said. \"The reasons it was quicker than people expected was the pace of the innovation going on with the underlying algorithms and also how much more potential you can get by combining different algorithms together.\" Image copyright Thinkstock Image caption DeepMind played with a full-sized board of 19 rows and 19 columns 'Major breakthrough' Prof Zoubin Ghahramani, of the University of Cambridge, said: \"This is certainly a major breakthrough for AI, with wider implications. \"The technical idea that underlies it is the idea of reinforcement learning - getting computers to learn to improve their behaviour to achieve goals. \"That could be used for decision-making problems - to help doctors make treatment plans, for example, in businesses or anywhere where you'd like to have computers assist humans in decision making. \"It doesn't mean that Google is ahead of all other companies in AI - there are many artificial intelligences. \"But in terms of devoting resources to Go, Google has clearly done more. \"Facebook has achieved some pretty spectacular results in other areas of artificial intelligence, but I think Google has beaten them to this particularly important challenge.\" Computer games DeepMind now intends to pit AlphaGo against Lee Sedol - the world's top Go player - in Seoul in March. Image copyright Google Image caption One of DeepMind's AI programs taught itself how to play the video game Breakout In addition, it continues to develop AI systems that can play computer games without any help, following last year's success at getting its bots to teach themselves how to play several dozen classics. \"For us, Go is the pinnacle of board game challenges,\" said Mr Hassabis. \"Now, we are moving towards 3D games or simulations that are much more like the real world rather than the Atari games we tackled last year. \"",
       "Computational encroachment and dominance of the human pastime of board games is almost complete, with one of the few remaining vestiges of human supremacy taking a beating with Google's AlphaGo system besting the European Go champion by five games to nil. Due to the sheer amount of moves available on a Go board -- 10^761 possible games, versus 10^120 for chess -- the game has been seen as harder for computers to crack, and almost impossible by the brute force methods used to conquer many other board games. Published in Nature today, Google has taken the wraps off its AlphaGo system, which trades raw power for a more nuanced approach involving a pair of neural networks and a new search algorithm. \"The key to AlphaGo is reducing the enormous search space to something more manageable,\" David Silver and Demis Hassabis, from Google DeepMind, said in a blog post. \"One neural network, the 'policy network', predicts the next move, and is used to narrow the search to consider only the moves most likely to lead to a win. \"The other neural network, the 'value network', is then used to reduce the depth of the search tree -- estimating the winner in each position in place of searching all the way to the end of the game.\" AlphaGo uses a Monte-Carlo tree search to look ahead to possible moves, with the neural networks suggesting moves and judging the board position. The system was trained on 30 million moves from games played by human experts, Google said, until it could predict the human move 57 percent of the time. Then it played against itself thousands of times. Against existing Go programs, AlphaGo won all but one of 500 games, and in October beat 2-dan professional player Fan Hui 5-0 -- replays of which can be viewed -- with a March match locked in against 9-dan professional Lee Sedol. Google said the most significant part of AlphaGo was not mastering Go, but rather using general-purpose learning techniques that could be applied to climate modelling or disease analysis. \"While games are the perfect platform for developing and testing AI algorithms quickly and efficiently, ultimately, we want to apply these techniques to important real-world problems,\" Silver and Hassabis said. \"Because the methods we have used are general purpose, our hope is that one day, they could be extended to help us address some of society's toughest and most pressing problems.\" Artificial intelligence systems beating humans at games entered popular consciousness when IBM's Deep Blue system was able to defeat Gary Kasparov in the late '90s and early 2000s. One of the towers that made up Deep Blue is now in the Smithsonian Museum. In 2011, IBM's Watson computer beat two of the most successful Jeopardy players of all time. Watson is currently being used primarily in healthcare.",
       "Share This article Google acquired the British artificial intelligence startup DeepMind just over two years ago, but at the time it wasnt clear what the secretive company was working on. Most of DeepMinds work has been under the radar, but Google has now announced DeepMinds research has led to a significant AI milestone. A new program called Alpha Go has been developed that can beat a professional human player at the game of Go, something no computer has managed to do before. Were all familiar with chess-playing computers  Deep Blue famously beat Garry Kasparov 20 years ago. Go, which was created more than 2,500 years ago in China, is considered a more significant challenge for AI because the overwhelming complexity makes it an intuitive game. The goal in Go is to place your pieces on the board to surround and capture the opponents pieces until you control more than half of the board. Its a game of pattern recognition and skill; theres no luck involved, making it a perfect problem to test artificial intelligence. The complexity of Go comes from the huge number of board configurations. In chess, there are only 32 total pieces and 64 squares on the board. Additionally, each piece can only move in certain ways. Its possible for a computer to brute force all the potential board configurations and plan many moves in advance. Go is played with identical pieces that are placed on a 19 by 19 grid (361 potential locations). The number of board configurations is huge  more than the number of atoms in the universe, making it impossible for a computer to simply brute force the search space. You need a computer that can learn to play the game like a human, and thats what Alpha Go did. Most serious Go players cant explain exactly why certain moves are the right ones  thus, the intuitive aspect. Most programmers felt until recently that Go was so complex it would take decades for a computer to best a human. Then, Alpha Go defeated European Go champion Fan Hui five games to zero in a recent match. This coming March, Alpha Go will take on Lee Sedol, one of the best players in the world. Google isnt the only AI company that has been interested in cracking Go, and now that it has, many of the same techniques could be applied to other problems. DeepMind researchers developed general AI methods, so theyre not locked into only playing Go; that would not make for a very useful AI. There are two basic learning networks inside Alpha Go  one network learns to predict likely upcoming moves and the other predicts the outcome of different arrangements of game pieces. It doesnt try to simulate an entire game with all the uncountable board configurations, but instead just thinks a few moves ahead like a human player would. With a different data set, these algorithms could tackle big problems like medical diagnosis and climate modeling. For now, DeepMind is focusing on the match with Lee Sedol. Alpha Go can run through millions of games per day to improve its understanding of the game. That might help it win more games, but playing games is just the beginning.",
       "Photo Artificial intelligence researchers are closing in on a new benchmark for comparing the human mind and a machine. On Wednesday, DeepMind, a research organization that operates under the umbrella of Alphabet, reported that a program combining two separate algorithms had soundly defeated a high-ranking professional Go player in a series of five matches. The result, which appeared in the Jan. 27 edition of the journal Nature, is further evidence of the power created when a class of A.I. machine learning programs known as deep neural networks is combined with immense sets of data. Go is seen as a good test for artificial intelligence researchers because it is more complex than chess, with a far larger range of possible positions. This makes strategy and reasoning in the game more challenging. Go is played with round black and white stones, and two players alternately place pieces on a square grid with the goal of occupying the most territory. Until recently, software programs had not been able to do better than beat amateur Go players. In the Nature paper, engineers at DeepMind described a program, AlphaGo, that had achieved a 99.8 percent winning rate against other Go programs. It also swept five games from the European Go champion, Fan Hui. The match between the AlphaGo program and Fan Hui was in October, and the DeepMind program has continued to train since then, said Demis Hassabis, a researcher who founded DeepMind Technologies, which was acquired by Google in 2014. Google changed its name to Alphabet last year, though the companys traditional ad-based businesses still operate under the Google label. The machine has continued to get better. We havent hit any kind of ceiling yet on performance, he said. The Alphabet approach relies on the newest so-called deep learning approach combined with a more traditional type of algorithm known as a Monte Carlo, which is designed to exhaustively explore large numbers of possible combinations of moves. The researchers said they had also trained their program using input from expert human Go players. The research and the game have created a rivalry among the public relations departments of companies like Alphabet, Microsoft and Facebook. The day before the Alphabet paper was published, Facebook republished an earlier paper the company had posted on the arXiv.org website. At the same time, Facebook issued blog posts from Yann LeCun, one of its artificial intelligence researchers, and one from the companys chief executive, Mark Zuckerberg. The statement by Mr. Zuckerberg resulted in a swift response from one Facebook user that may express a deeper human concern than the narrow results of the research: Why dont you leave that ancient game alone and let it be without any artificial players? Do we really need an A.I. in everything? wrote Konstantinos Karakasidis. Those concerns are not likely to be heeded. In a blog post Wednesday morning, Alphabet stated that, in an effort to reprise the winning IBM Deep Blue chess playing program that defeated the chess champion Garry Kasparov in 1996, Alphabet will match its AlphaGo program against Lee Sedol, the current Go champion, for a five-game match in March. There will be a $1 million prize for the winner, and Mr. Hassabis said that Alphabet would donate the prize to charity if AlphaGo won. The match will be streamed live on YouTube. Mr. Hassabis, who is a skilled chess player and has been a professional gamer as well, said that Go was a beautiful game, but that building an A.I. is also a human endeavor and a kind of ingenious one, too. The reason games are used as a testing ground is that theyre kind of like a microcosm of the real world.",
       "For the first time ever, a computer has beaten a professional Go player, Fan Hui. This is a major breakthrough when it comes to artificial intelligence and neural networks, as beating top Go players has been the last symbolical challenge. Google DeepMinds AlphaGo is much more sophisticated than IBMs Deep Blue, the chess-playing computer developed by IBM. Arguably, were now living in a golden age of artificial intelligence. Go is a board game for two players. Despite its relatively simple rules and the fact that Go has been around for 2,500 years, top players are still looking for the best strategies. In particular, every time you play, there are hundreds of possible moves. Players are talking about finding the most elegant shape to build territories. In many ways, compared to chess, Go is an open-ended game with multiple ways of playing, reflecting your personality. I first learned to play Go 15 years ago. At the time, I could beat the best bots  and believe me, I wasnt a good player. Go bots have come a long way. Thats why todays news is surprising and fascinating at the same time. Many companies including Google and Facebook have been building teams of artificial intelligence researchers to work on the future. There are obvious applications, such as facial recognition, self-driving cars, Google Now, M for Facebook Messenger and more. But these teams are also working on more trivial issues, such as Go bots, in order to learn new techniques. And it looks like DeepMind is one step ahead of the competition with AlphaGo. It combines the Monte-Carlo technique to evaluate the most efficient moves in the tree of potential moves with neural networks. DeepMind also uses external memory resources to boost the efficiency of neural networks. In more pragmatical terms, AlphaGo is a bot that learns over time. The more games it plays, the better it gets. AlphaGo is even playing games against itself to get better. A few days ago, DeepMind played against European Go champion Fan Hui. AlphaGo won by 5 games to 0. In March, AlphaGo will play against the best player in the world, Lee Sedol. Now, its not a matter of knowing if AlphaGo can beat Lee Sedol, but when. As I wrote on Twitter, the difference between our good old monkey-derived brains and computer programs is getting smaller. At some point, artificial intelligence is going to become intelligence, like new technologies became technology. Featured Image: Jaro Larnos/Flickr UNDER A CC BY 2.0 LICENSE",
       "In 1997, IBMs supercomputer Deep Blue famously defeated reigning chess champion Garry Kasparov in a heavily publicized match that marked a significant breakthrough in the field of machine learning. Two decades on, computers have learned to master backgammon and several Atari video games, such as Breakout, Pinball, Space Invaders and Pong. Now, in another milestone in machine learning and pattern recognition, a computer program developed by researchers at the Alphabet-owned Google DeepMind division defeated the three-time European champion of the ancient Chinese game Go  which has long been considered the most challenging game for a computer to master. The team that developed the computer program, named AlphaGo, detailed the findings in an article published in the journal Nature. Go is believed to have been invented in China nearly 2,500 years ago. Its played by placing black or white stones on a square grid. When a player surrounds any of his opponents pieces, theyre captured. The goal of the game is to control at least 50 percent of the board. While the rules of the game are simpler than those of chess, the overall complexity is much higher, making it, in the words of DeepMind co-founder Demis Hassabis, a much more intuitive game. Go is a game of profound complexity, AI researcher Hassabis wrote in a blog post Wednesday. This complexity is what makes Go hard for computers to play, and therefore an irresistible challenge to AI researchers, who use games as a testing ground to invent smart, flexible algorithms that can tackle problems, sometimes in ways similar to humans. As DeepMinds AI researchers explain, while chess offers some 20 possible choices per move, Go has about 200. In other words, the game provides more possible positions than there are atoms in the universe. As a result, computer scientists have been trying to crack the game for years. Coincidentally, just a day before the DeepMind team announced its breakthrough, Facebook founder Mark Zuckerberg wrote in a post that his companys AI team was getting close to achieving the same thing. The ancient Chinese game of Go is one of the last games where the best human players can still beat the best artificial... Posted by Mark Zuckerberg on Tuesday, 26 January 2016 Tested against rival Go-playing AIs, Google's system won 499 out of 500 matches. And, last October, when AlphaGo was pitted against Fan Hui  Europes top player the program won all five games. The most significant aspect of all this for us is that AlphaGo isnt just an expert system built with hand-crafted rules; instead it uses general machine learning techniques to figure out for itself how to win at Go, Hassabis wrote, in the blog post. While games are the perfect platform for developing and testing AI algorithms quickly and efficiently, ultimately we want to apply these techniques to important real-world problems. Since the methods used by AlphaGo to master Go are general purpose, in the future, the company hopes to be able to use the new computing power in fields like healthcare, complex disease analysis, and climate modeling. For now, though, the researchers at DeepMind are preparing for another test  pitting AlphaGo against the world's top Go player Lee Sedol in Seoul in March.",
       "Google has achieved something major in artificial intelligence (AI) research. A computer system it has built to play the ancient Chinese board game Go has managed to win a match against a professional Go player: the European champion Fan Hui. The research is documented in a paper in this weeks issue of the journal Nature. The Google system, named AlphaGo, swept Frances Hui, who is ranked a 2-dan, in a five-game match at the Google DeepMind office in London in October. AlphaGo played against Hui on a full 19-by-19 Go board and received no handicap. Now, Google is preparing to put AlphaGo up against the highest-ranked Go player, South Koreas Lee Sedol, at a match in Seoul in March. If we win the match in March, then thats sort of the equivalent of beating [Garry] Kasparov in chess, said Demis Hassabis, cofounder of Google-owned DeepMind, during a press briefing on the research earlier this week. Lee Sedol is the greatest player of the past decade. I think that would mean AlphaGo would be better than any human at playing Go. Go aint easy With its white and black pieces that players place on a board with a grid, Go bears some resemblance to chess. And chess has been a focus of AI research for decades. But even though Gos gameplay is simpler than chess, Go poses more difficult challenges for intelligence both artificial and human because there are many more possible moves that a player can make at any given turn, and many more possibilities for the outcome of the game as a result. Image Credit: Nature An expert system such as IBMs Deep Blue computer, which beat chess grandmaster Kasparov in 1997, just wont scale for this problem set. So Googles brightest minds have brought together an ensemble of AI techniques in order to succeed in this domain. Overall, Googles DeepMind is calling on a type of AI called deep learning, which involves training artificial neural networks on data  such as photos  and then getting them to make inferences about new data. Google has picked up plenty of talent in the area through acquisitions  DNNresearch, Jetpac, and certainly DeepMind. And deep learning is working inside several Google services, from Google Photos to Google Translate. Baidu, Facebook, and Microsoft all conduct research on deep learning and use it in their own products as well. Coincidentally (or not), Facebook yesterday published a paper on its own progress in the problem set of using AI to play Go. But unlike Google, Facebook has not succeeded in having its AI win a Go match against a professional player. How it works Googles system relies on many components. First, to predict which moves to play next, Google trains a 13-layer policy network on data reflecting expert Go players moves in games. And its data at scale  30 million positions from the widely used KGS Go Server. Google enhances this policy network with reinforcement learning, which uses a process of trial and error. Effectively, it gets smarter by playing against itself. Image Credit: Nature paper screenshot Then, Google trains a value network that can predict which side will win a game. Google takes these two deep neural networks and brings them together with a Monte Carlo tree search, which is commonly used for Go bots. From there, the Google AI is ready to play. Testing the system The DeepMind researchers additionally had AlphaGo play against other Go AI programs, like Crazy Stone, Fuego, Pachi, and Zen. Out of 495 matches, AlphaGo won 494. Even when the researchers gave a handicap to the other programs, AlphaGo generally was able to win. Unsurprisingly, the program became even better when the researchers scaled out AlphaGo across multiple servers in a distributed fashion. And at least one of the Google DeepMind researchers also tried his hand at beating the system. In the early days of AlphaGo I played against it, but it was quickly apparent that AlphaGo was way beyond my skill level, DeepMinds David Silver said in an interview for a video Nature produced in association with the paper. What comes next Google can do a lot with the AlphaGo technology. Perhaps it could be used to help amateur Go players improve their skills, especially in areas where you cant easily find a Go teacher nearby. Image Credit: Google Practically speaking, within a year or two, AlphaGos core capabilities could be brought to bear inside of Google services, Hassabis said. But hes thinking beyond that. Ultimately we want to apply these techniques in important real-world problems, such as climate modeling or medical diagnostics, Hassabis said. But in keeping with the agreement that DeepMind made with Google during the acquisition in 2013, the AI technology will never be used for military purposes, he said. As for what game the DeepMind team might take on after Go (and the Atari 2600), no-limit poker may be an option, Hassabis said. See the Nature paper and Hassabis and Silvers post on the Google Research blog for more detail on the Go work. More information: Google Google's innovative search technologies connect millions of people around the world with information every day. Founded in 1998 by Stanford Ph.D. students Larry Page and Sergey Brin, Google today is a top web property in all major glob... read more  Powered by VBProfiles",
       "Artificial intelligence researchers at Google DeepMind are celebrating after reaching a major breakthrough thats been pursued for more than 20 years: The team taught a computer program the ancient game of Go, which has long been considered the most challenging game for an an artificial intelligence to learn. Not only can the teams program play Go, its actually very good at it. The computer program AlphaGo was developed by Google DeepMind specifically with the task of beating professional human players in the ancient game. The group challenged the three-time European Go Champion Fan Hui to a series of matches, and for the first time ever, the software was able to beat a professional player in all five of the games played on a full-sized board. The team announced the breakthrough in a Nature article published today. Advertisement Coincidentally, just one day before the Google DeepMind team announced its scientific achievement, Facebook CEO Mark Zuckerberg wrote a public Facebook post saying that his AI team is getting close to achieving the same exact thing. He wrote that the researcher who works on this, Yuandong Tian, sits about 20 feet from his desk, and added, I love having our AI team right near me so I can learn from what theyre working on. Facebooks competitor to Googles AlphaGo is called Darkforest. Yuandong Tian published the name in November, when he submitted a paper to the International Conference on Learning and Representations. Sponsored Mark Zuckerberg posted this video illustrating Facebooks research. The history of Go dates back to ancient China, some 2,500 years ago. Its played by placing black or white stones on a 19 x 19 grid. When a player surrounds any of his opponents pieces, theyre captured. The goal of the game is to control at least 50 percent of the board. The reason that its so difficult for computers to play is because there is an estimated 10 to the power of 700 possible variations of the game. By comparison, chess only has 10 to the power of 60 possible scenarios. The breakthrough achieved by Google DeepMind is important for several reasons: Broadly speaking, it will impact the way that computers are able to search for a sequence of actions. That will help AI programs get from one place to another and navigate through logic. To the average person, that can mean a lot of different things because theyre often asking an artificial intelligence to get from one place to another by reasoning through a set of logic equations. More specifically, things like facial-recognition processing and predictive search are the most easy advances to point to. Both Facebook and Google trade on the very ability to analyze data better than other companies, and ultimately, to create and sell products based on that analysis. The data is, to be more specific, who you are and everything you do. Following this announcement, the Google DeepMind team has issued a challenge to the best player in the world, Lee Sedol of South Korea, who has long been considered the greatest player of the modern era. The match is scheduled to take place in March 2016. The Sedol vs. AlphaGo match will have many similarities to the famous 1996 chess match between chess grandmaster Gary Kasparov and IBMs Deep Blue computer. In that match, IBMs Deep Blue artificial intelligence was the first to defeat a professional chess grandmaster. In the upcoming match between Sedol and AlphaGo, DeepMinds artificial intelligence will have to sort through a much larger decision tree than IBMs Deep Blue in addition to sorting through a higher number of moves. Keep going at it! This is some fun competition. Top image via Flickr",
       "Image copyright Getty Images Image caption The board game Go is thought to have been played for thousands of years Facebook has created an artificial intelligence system that is \"getting close\" to beating the best human players at the Chinese board game Go, Mark Zuckerberg has revealed. The social network's founder added that the work was being done close to his desk, signalling the importance he is giving to the task. One expert said the challenge could result in far-reaching benefits. It is the second time Mr Zuckerberg has highlighted work on AI this month. Facebook is far from the only tech firm to have used computers to play Go - a game with trillions of possible moves. Microsoft's research division began developing AI software to tackle the issue in 2004, and ended up releasing an Xbox video game six years later that made use of its techniques. Google's AI chief Demis Hassabis has also indicated that his DeepMind team is working on the game. Image copyright Facebook Image caption Facebook believes its AI software would rank as an advanced amateur if it were human Go is thought to have first been played more than 2,500 years ago in ancient China. Two people take turns to place black or white stones on to a grid, with the goal being to dominate the board by surrounding the opponent's pieces. Once placed, the stones cannot be moved unless they are surrounded and captured by the other person's pieces. It has been estimated that there are 10 to the power of 700 (10 multiplied by itself 699 times) possible ways a Go game could be played. By contrast, chess - a game at which AIs can already play at grandmaster level - has about 10 to the power of 60 possible scenarios. Image copyright Microsoft Research Image caption Microsoft Research used its work on Go to power an Xbox video game \"Scientists have been trying to teach computers to win at Go for 20 years,\" wrote Mr Zuckerberg on his Facebook page. \"We're getting close, and in the past six months we've built an AI that can make moves in as fast as 0.1 secs and still be as good as previous systems that took years to build. \"Our AI combines a search-based approach that models every possible move as the game progresses along with a pattern matching system built by our computer vision team. \"The researcher who works on this, Yuandong Tian, sits about 20ft from my desk. I love having our AI team right near me so I can learn from what they're working on.\" Pointless moves Facebook's Go AI system is codenamed Darkforest, according to a paper submitted in November by Mr Tian to the International Conference on Learning Representations. He wrote that it had achieved a \"stable\" five dan level in the game, representing an advanced amateur but below the \"professional levels\". Image copyright Getty Images Image caption Mark Zuckerberg's Messenger Platform is developing AI to help it make recommendations However, he acknowledged the software still had flaws. \"Sometimes the bot plays tenuki (\"move elsewhere\") pointlessly when a tight local battle is needed,\" Mr Tian wrote. \"When the bot is losing, it shows the typical behaviour of MCTS [a machine learning technique known as Monte Carlo Tree Search], that plays bad moves and loses more. We will improve these in the future.\" Facebook is currently trying to use other AI systems to answer questions and carry out tasks on its Messenger chat platform. 'Harder than chess' Mr Zuckerberg has also made public his desire to build a \"simple AI\" to power his home and help him at work this year. One independent researcher said the firm's work on Go could have knock-on benefits. \"Go is vastly harder than chess using any of the standard techniques for game-playing, and for some time it's been the case that people have regarded that you would need something fundamentally new in order to crack it,\" explained Dr Sean Holden from the University of Cambridge's computer laboratory. \"Playing games like this is essentially a search problem. The AI has to search for a sequence of actions that will get you from the start of the game to a winning position. \"And that general search problem is potentially usable in all manner of different AI scenarios. \"Because, what AI essentially comes down to is that if you have a robot and you want it to achieve a task, you want it to find a sequence of moves from where it is to where you want it to be.\"",
       "Black-and-white pieces occupy spaces on a board during a game of Go, which Google's software engineers say they've taught a computer program to play better than most humans. (Photo: Courtesy Google) Googles software engineers have taught a computer program to beat almost any human at an ancient and highly complex Chinese strategy game known as Go. While computers have largely mastered checkers and chess, Go, considered the oldest board game still played, is far more complicated. There are more possible positions in the game than are atoms in the universe, Google said  an irresistible challenge for the companys DeepMind engineers, who used artificial intelligence enable the program to learn from repeat games. The Google unit's AlphaGo computer program is much more sophisticated than the IBM-created Deep Blue computer that in 1996 won the first chess game against a reigning world champion, Garry Kasparov. The AlphaGo system makes what its developers consider to be fewer, but smarter, decisions. Previous systems relied much more on whats known as brute force calculations. In other words, Deep Blue and its contemporaries used massive processors to plot out millions of possible moves in a relatively short time. The game of Go is harder to mathematically predict, in part because the board is much larger: While a chessboard has 64 squares, Go has 361. The brute force-style approach \"has led to superhuman performance in chess, checkers and Othello, but it was believed to be intractable in Go due to the complexity of the game, AlphaGos creators wrote in a paper published Wednesday in the journal Nature. Until now, the best computer Go players were no better than amateur humans. The object of Go is to control as much of the the board as possible, and players use either white or black stones to surround territory and their opponent. The new approach worked by teaching the program how humans played, and then letting its learning software play game after game for practice. Ultimately it defeated the reigning European champion 5-0 in October. Its the first time a computer has beaten a professional player in a complete game, the Google developers said. AlphaGo's next challenge will be playing the worlds top Go player in March. Zuckerberg also posts on Go Artificial intelligence is undergoing a major boom in Silicon Valley. Alphabet-unit Google is a leader in machine learning and deep learning, and rivals including Facebook and Microsoft are also making substantial investments. Google, which bought DeepMind in early 2014, is using artificial intelligence to improve its products and services by training computers to learn from data with little or sometimes no human intervention in areas such as search, translation and photo storage. Smarter, more powerful computers could help Google's search engine learn and improve results in real time as people click on answers or Web links, for example. And Facebook uses artificial intelligence to automatically recognize and label friends in photos posted on the social network. In a lengthy blog post Wednesday, Facebook CEO Mark Zuckerberg lauded the promise of artificial intelligence but said scientists still haven't figured out how to get a computer to learn like a human, and then apply lessons learned in one area to another. In a separate post made Tuesday, Zuckerberg discussed Facebook's efforts to teach a computer to play Go but doesn't mention Google's competing effort. \"We should not be afraid of AI. Instead, we should hope for the amazing amount of good it will do in the world,\" Zuckerberg wrote. \"It will saves lives by diagnosing diseases and driving us around more safely. It will enable breakthroughs by helping us find new planets and understand Earth's climate. It will help in areas we haven't even thought of today.\" Read or Share this story: http://usat.ly/1Vs54Il"
   ],
   "common_words": [
       "game"
   ],
   "top_image": {
     "low":"http://i68.servimg.com/u/f68/13/85/87/18/_8796110.jpg",
     "original":"http://ichef-1.bbci.co.uk/news/1024/cpsprodpb/973C/production/_87961783_87961779.jpg"
   },
   "summary": [
       "Google's DeepMind division has achieved a landmark in AI A Google artificial intelligence program has beaten the European champion of the board game Go.",
       "You need a computer that can learn to play the game like a human, and thats what Alpha Go did.",
       "The ancient Chinese game of Go is one of the last games where the best human players can still beat the best artificial... Posted by Mark Zuckerberg on Tuesday, 26 January 2016 Tested against rival Go-playing AIs, Google's system won 499 out of 500 matches.",
       "For now, though, the researchers at DeepMind are preparing for another test pitting AlphaGo against the world's top Go player Lee Sedol in Seoul in March.",
       "Facebook is far from the only tech firm to have used computers to play Go- a game with trillions of possible moves.",
       "Ultimately we want to apply these techniques in important real-world problems, such as climate modeling or medical diagnostics, Hassabis said.",
       "The group challenged the three-time European Go Champion Fan Hui to a series of matches, and for the first time ever, the software was able to beat a professional player in all five of the games played on a full-sized board."
   ],
   "tweets": [
           {
           "id": 693722397543825412,
           "text": "Can Google’s AlphaGo really feel it in its algorithms? | John Naughton http://d.gu.com/KMZDrP",
           "name": "Guardian Science",
           "username": "@guardianscience",
           "blockquote": "",
           "link": "https://twitter.com/guardianscience/status/693722397543825412"
         },
         {
         "id": 693813283015528448,
         "text": "Google Great Gains in the Grand Game of Go #DeepMind #AlphaGo @mattmayo13 http://buff.ly/23E8T39",
         "name": "Gregory Piatetsky",
         "username": "@kdnuggets",
         "blockquote": "",
         "link": "https://twitter.com/kdnuggets/status/694212445687136256"
         },
         {
         "id": 693256141774655488,
         "text": "Google AlphaGo AI clean sweeps European Go champion http://zd.net/1JGidNi  via @ZDNet & @dobes",
         "name": "ZDNet",
         "username": "@ZDNet",
         "blockquote": "",
         "link": "https://twitter.com/ZDNet/status/693256141774655488"
         }
           ],
           "quotes": [
                      {
                      "person": "David Silver",
                      "text": "The key to AlphaGo is reducing the enormous search space to something more manageable"
                      }
                      ],
           "products": [
             {
             "name": "Google DeepMind",
             "source_name": "wikipedia",
             "source_url": "https://en.wikipedia.org/wiki/Google_DeepMind",
             "text": "Google DeepMind is a British artificial intelligence company founded in 2010 as DeepMind Technologies. It was renamed when it was acquired by Google in 2014. "
           },
           ],
           "companies": [
             {
             "name": "Google",
             "source_name": "wikipedia",
             "source_url": "https://en.wikipedia.org/wiki/Google",
             "text": "Google Inc. is an American multinational technology company specializing in Internet-related services and products. These include online advertising technologies, search, cloud computing, and software."
           },
           {
           "name": "Alphabet",
           "source_name": "wikipedia",
           "source_url": "https://en.wikipedia.org/wiki/Alphabet_Inc.",
           "text": "Alphabet Inc. (commonly known as Alphabet, and frequently informally referred to as Google) is an American multinational conglomerate created in 2015 as the parent company of Google and several other companies previously owned by or tied to Google."
           }
           ],
           "people": [],
           "organizations": [],
           "locations": [
                         "San Francisco"
                         ]
}]
